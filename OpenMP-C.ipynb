{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Welcome to the Parallel C OpenMP hand-on session. \n",
    "\n",
    "#### In this Jupyter Notebook, you are provided with four scientific programming tasks that can be accelerated by OpenMP multithreading. Each example highlights a distinct loop-parallelisation theme and showcases different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "# the jupyter notebook is launched from your $HOME, change the working directory provided a username directory is created under /scratch/vp91\n",
    "os.chdir(os.path.expandvars(\"/scratch/vp91/$USER/OpenMP-C\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1. (Independent Loop Iterations) Monte-Carlo $\\pi$\n",
    "\n",
    "Consider the quarter of a unit circle inscribed in the unit square $[0,1]^2$. Draw $N$ points uniformly at random in the square, and let $h$ be the number that fall inside the quarter circle $(x^2+y^2\\le 1)$. By the area ratio,\n",
    "\\begin{align*}\n",
    "\\hat{\\pi} \\approx \\frac{4h}{N}.\n",
    "\\end{align*}\n",
    "\n",
    "Increasing $N$ reduces the estimator’s variance (error estimate is $O(N^{-1/2})$). Because each sample is independent, this Monte Carlo approach is *embarrassingly parallel*, making it an ideal example for OpenMP Multithreading paralelisation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the python-based Jupyter environment you are working on, we need to prepend lines of code with `!` to indicate we wish to execute a shell command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.0\n",
    "Inspect the serial codebase [monte-carlo-pi-serial.c](./monte-carlo-pi-serial.c). Idenity the for-loop where OpenMP multithreading is suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilee the serial code\n",
    "!gcc -g -Wall -O3 -o monte-carlo-pi-serial monte-carlo-pi-serial.c -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Execute the serial code\n",
    "!./monte-carlo-pi-serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: \n",
    "Add OpenMP work-sharing loop constructs and appropriate clauses to [monte-carlo-pi-omp.c](./monte-carlo-pi-omp.c) to parallelise the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the openmp code\n",
    "!gcc -fopenmp -g -Wall -O3 -o monte-carlo-pi-omp monte-carlo-pi-omp.c -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Execute the openmp code, compare the time consumption with the serial version\n",
    "!OMP_NUM_THREADS=4 ./monte-carlo-pi-omp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway from Exercise 1:\n",
    "If your workload is embarrassingly parallel (independent iterations, little/no communication), OpenMP is an excellent first choice on a single shared-memory machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2 (Dynamic Loop Iterations) Mandelbrot Set\n",
    "## Mandelbrot Set\n",
    "\n",
    "The **Mandelbrot set** is the set of complex numbers $c \\in \\mathbb{C}$ for which the sequence\n",
    "\n",
    "$$z_{n+1} = z_n^2 + c,\\qquad z_0 = 0$$\n",
    "remains **bounded** (does not diverge to infinity).\n",
    "\n",
    "We write\n",
    "$$\n",
    "M \\;=\\; \\Big\\{\\, c \\in \\mathbb{C} \\;\\big|\\; \\{z_n\\}_{n\\ge 0}\\ \\text{is bounded for } z_{n+1}=z_n^2+c,\\ z_0=0 \\,\\Big\\}.\n",
    "$$\n",
    "\n",
    "### Convergence (Escape) Test\n",
    "For visualization, we sample $c$ on a uniform grid over the rectangle\n",
    "$$\n",
    "[-2.0,\\ 0.47] \\times [-1.12,\\ 1.12]\\,i,\n",
    "$$\n",
    "which covers the most visually interesting portion of $M$.\n",
    "\n",
    "For each grid point $c$, iterate the map up to $N_{\\max}=100$ steps:\n",
    "$$\n",
    "z_{n+1} = z_n^2 + c,\\quad n=0,\\dots, N_{\\max}-1.\n",
    "$$\n",
    "If at any step $|z_n| > 2$ (equivalently $|z_n|^2 > 4$), the orbit is guaranteed to diverge and $c \\notin M$. If $|z_n|$ never exceeds the value $2$ within $N_{\\max}$ steps, we treat $c$ as (numerically) belonging to $M$.\n",
    "\n",
    "### Dynamic Workload\n",
    "Different points require different numbers of iterations before diverging (some escape in a few steps, others never escape within $N_{\\max}$). This **variable iteration count** creates load imbalance in parallel implementations. To mitigate this with OpenMP, use **dynamic scheduling** (e.g., `schedule(dynamic, chunk)`) so threads pick up new points as they finish, improving overall utilization.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.0:\n",
    "Inspect the serial codebase [mandelbrot-serial.c](./mandelbrot-serial.c).\n",
    "Compile and run the serial version of mandelbrot set generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the serial code\n",
    "\n",
    "!gcc -Wall -O3 mandelbrot-serial.c -o mandelbrot-serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the serial program \n",
    "!./mandelbrot-serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mandelbrot set to verify outputs\n",
    "%run  'Mandelbrot-plot.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "Add OpenMP work-sharing loop constructs and appropriate clauses to [mandelbrot-omp.c](./mandelbrot-omp.c) to parallelise the computation. Ensure that writes to the output file remain in order (e.g., with #pragma omp ordered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the openmp code\n",
    "!gcc -fopenmp -g -Wall -O3 -o mandelbrot-omp mandelbrot-omp.c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Execute the openmp code\n",
    "\n",
    "!OMP_NUM_THREADS=4  ./mandelbrot-omp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Verify the results by plot\n",
    "%run  'Mandelbrot-plot.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve probably noticed the OpenMP version doesn’t run faster. The culprit is the use of `ordered` around your output.\n",
    "\n",
    "**What `ordered` does:**  \n",
    "`ordered` enforces that a specific block inside a `parallel for` executes **in the original loop order**. Only one iteration at a time may enter that block. In other words, it **serializes** the part it wraps.\n",
    "\n",
    "**What to do instead (best practice)**\n",
    "- **Keep compute parallel, do I/O serially.**  \n",
    "  Compute the iteration counts into a preallocated buffer in the `parallel for` and write the data after the parallel region.\n",
    "\n",
    "\n",
    "### Exercise 2.2 \n",
    "Modify [mandelbrot-separateIO.c](./mandelbrot-separateIO.c) to separate the I/O from the computation routine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the openmp code after IO separation\n",
    "!gcc -fopenmp -g -Wall -O3 -o mandelbrot-separateIO mandelbrot-separateIO.c \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    " \n",
    "!OMP_NUM_THREADS=4  ./mandelbrot-separateIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the output again by plot\n",
    "%run  'Mandelbrot-plot.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway from Exercise 2:\n",
    "When planning acceleration, consider refactoring the code to fit your parallelisation tools. Here, we separated I/O from computation so the compute kernel could be parallelised cleanly with OpenMP (no I/O inside parallel regions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #3 (Loop Dependence). Solve Linear Equation by Conjugate Gradient Method \n",
    "\n",
    "The Conjugate Gradient method is a numerical method that is wildely used in solving certain type of matrix problems. It is also the base ingredient of [HPCG Benchmark](https://www.hpcg-benchmark.org/) for ranking HPC systems.\n",
    "\n",
    "\n",
    "Consider solving a linear equation\n",
    "\\begin{align*}\n",
    "Ax = b\n",
    "\\end{align*}\n",
    "where matrix $A \\in \\mathbf{R}^{n\\times n}$ is symmetric positive definite. \n",
    "\n",
    "The initial guess $x_0$ can be any approximation, we choose $0$. The baseline algorithm is statedd as following:\n",
    "\n",
    "Compute $r_0 = b - Ax_0$\n",
    "\n",
    "For $i= 0, \\cdots, n$ Do \n",
    "\n",
    " $\\alpha_i := (r_i, r_i)/(Ap_i, p_i)$\\;\n",
    " \n",
    " $x_{i+1}:=x_i+\\alpha_i p_i$\\;\n",
    "\n",
    " $r_{i+1}:=r_i -\\alpha_i Ap_i$\\;\n",
    " \n",
    " If $r_{i+1} <\\text{tolerance}$ Then Break\n",
    " \n",
    " $\\beta_i:= (r_{i+1}, r_{i+1}) / (r_i, r_i)$\\;\n",
    " \n",
    " $p_{i+1}:= r_{i+1} +\\beta_i p_i$\n",
    "\n",
    "Conjugate Gradient (CG) method is a direct method that produces the exact solution at most $n$ steps, however, in practice a tolerance is usually set to terminate iterations.\n",
    "\n",
    "CG method guarantees convergence for symmetric positive definite matrices in theory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelisation in CG method\n",
    "Note that the for-loop in CG method possesses the dependence between iterations. With multiple threads, each iteration can only be execute only after the dependence is met leading to a non-parallelizable loop. \n",
    "\n",
    "That being said, we can still use OpenMP to parallelise part of the code after analysing the bottleneck of its performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Matrices\n",
    "For numerical experiments, we will test two matrices [Trefethen_20](https://www.cise.ufl.edu/research/sparse/matrices/JGD_Trefethen/Trefethen_20.html) and [Msc04515](https://www.cise.ufl.edu/research/sparse/matrices/Boeing/msc04515.html).\n",
    "\n",
    "Trefethen_20 is a small-sized problem in which you should see a fast convergence.\n",
    "\n",
    "Msc04515 is a real-life problem arising from a structural engineering. It is an ill-conditioned matrix, which essentially means hard to solve and requires a lot more iterations for CG method if it converges at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.0\n",
    "Inspect the serial code base [cg-std.c](./cg-std.c), and map each step of the pseudo-algorithm to its corresponding implementation in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the serial code\n",
    "\n",
    "!gcc -g -Wall -O3 -o cg-std cg-std.c -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the serial program to solve Trefethen_20 problem\n",
    "!./cg-std 1e-5 < Trefethen_20.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "Add the appropriate OpenMP work-sharing constructs and clauses to the loops in [cg-std-omp.c](./cg-std-omp.c) to  to parallelise execution and improve performance.\n",
    "\n",
    "Once you are one, compile the code with the following command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the openmp code\n",
    "!gcc -fopenmp -g -Wall -O3 -o cg-std-omp cg-std-omp.c -lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code with a simple matrix [Trefethen_20.dat](./Trefethen_20.dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Verify the numerical result with the serial version\n",
    "!OMP_NUM_THREADS=4  ./cg-std-omp 1e-5 < Trefethen_20.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the result looks good, test it with a more serious matrix [msc04515.dat](./msc04515.dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Excute the parallel code to solve Boeing msc04515 problem. You may also want to attempt this problem with the serial code\n",
    "\n",
    "!OMP_NUM_THREADS=4 ./cg-std-omp 1e-5 < msc04515.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results with a serial code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Excute the parallel code to solve Boeing msc04515 problem. You may also want to attempt this problem with the serial code\n",
    "\n",
    "!OMP_NUM_THREADS=4 ./cg-std 1e-5 < msc04515.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway from Exercise 3:\n",
    "You don’t always need to understand the entire codebase (though it often helps). Even in a large project, pinpointing the bottleneck and adding targeted parallelism can deliver significant speedups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #4 (Parallelisaton vs. Convergence Rate): Solve Finite Difference Discretised Poisson Equation by Jacobi and Gauss-Seidel Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Problem\n",
    "Consider a 2D Poisson equation with Dirichlet boundary condition over a unit square domain $\\Omega = [0,1] \\times [0,1]$\n",
    "\n",
    "\\begin{align*}\n",
    "-\\Delta u &= f \\; \\text{in} \\; \\Omega \\\\\n",
    " u &= g \\; \\text{on} \\; \\partial \\Omega\n",
    "\\end{align*}\n",
    "\n",
    "Define a uniform partition of the domain $\\Omega$ with nodal points at which the solution of the Poisson equation is sampled. Let $h$ be the uniform distance between two nodal points then the nodal points that lie on the mesh are defined by\n",
    "\n",
    "\\begin{align*}\n",
    "x_i = i h, \\; y_j = j h\\qquad i,j = 0,\\cdots, N\n",
    "\\end{align*}\n",
    "where $N$ is a given mesh size and $i, j$ are integers along $x, y$-axis telling the location of each nodal point.  \n",
    "\n",
    "\n",
    "### Discretisation\n",
    "We use the second-order central finite difference method to discretise the Laplace operator\n",
    "\\begin{align*}\n",
    "(\\Delta u)_{i,j}\n",
    "  &= \\bigl(D_{xx}^2 u\\bigr)_{i,j} + \\bigl(D_{yy}^2 u\\bigr)_{i,j} \\\\[2ex]\n",
    "  &\\approx \\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{h^2}\n",
    "          + \\frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{h^2}.\n",
    "\\end{align*}\n",
    "leading to\n",
    "\\begin{align}\n",
    "    -(\\Delta u)_{i,j} = \\frac{4u_{i,j}-u_{i+1,j}-u_{i-1,j}-u_{i,j+1}-u_{i,j-1}}{h^2}=f(u_{i,j}).\n",
    "\\end{align}\n",
    "\n",
    "The above finite-difference formula can further be represented by a five-point stencil matrix built in the mesh\n",
    "\\begin{align*}S = \n",
    "    \\begin{pmatrix}\n",
    "    & -1 & \\\\\n",
    "    -1 & 4 &-1\\\\\n",
    "    & -1 &\n",
    "    \\end{pmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "Impose the Dirichlet boundary condition, on the interior nodal points the discretisation can be written as a linear equation\n",
    "\n",
    "\\begin{align*}\n",
    "A u = f, \\qquad A=\\frac{1}{h^2}\n",
    "    \\begin{pmatrix}\n",
    "S & I \\\\\n",
    "I & S & I \\\\\n",
    "& I & \\ddots & \\ddots \\\\\n",
    "& & \\ddots & \\ddots & I \\\\\n",
    "& & & I & S\n",
    "\\end{pmatrix}\n",
    "\\end{align*}\n",
    "where $A \\in \\mathbb{R}^{(N-2)^2 \\times (N-2)^2}$, $u \\in \\mathbb{R}^{(N-2)^2}$ and $f \\in \\mathbb{R}^{(N-2)^2}$.\n",
    "\n",
    "Note that with the five-point stencil, the matrix $A$ was never assembled and is nowhere in sight! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Solvers\n",
    "\n",
    "To solve the linear system, two iterative methods are used and compared. \n",
    "\n",
    "$\\textbf{Jacobi method}$\n",
    "\n",
    "\\begin{align*}\n",
    "u^{(k+1)} = D^{-1}( f - Lu_{k} -Uu_k),\n",
    "\\end{align*}\n",
    "where $D, L, U$ are the diagonal matrix, lower triangular matrix and upper triangular matrix of $A$, respectively.\n",
    "Write into stecil,\n",
    "\\begin{align*}\n",
    "u^{(k+1)}_{ij} = (h^2 f_{ij} + u^{(k)}_{i-1,j} +u^{(k)}_{i+1,j} + u^{(k)}_{i, j-1}+u^{(k)}_{i,j+1})/4 \\qquad i,j = 1,\\cdots, N-1 \n",
    "\\end{align*} \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Gauss-Seidel method}$ follows a similar scheme:\n",
    "\n",
    "\\begin{align*}\n",
    "u^{(k+1)} = D^{-1} (f - L u_{k+1}- U u_k),\n",
    "\\end{align*}\n",
    "\n",
    "Note that Gauss-Seidel method uses the most recent estimate to update.\n",
    " \n",
    "Likewise, applying the Gauss-Seidel method doesn't require assembling $D, L, U$ matrices for finite-difference discretised Laplacian. \n",
    "Elementwise, we have \n",
    "\\begin{align*}\n",
    "u^{(k+1)}_{ij} = (h^2 f_{ij} + u^{(k)}_{i-1,j} +u^{(k+1)}_{i+1,j} + u^{(k+1)}_{i, j-1}+u^{(k)}_{i,j+1})/4 \\qquad i,j = 1,\\cdots, N-1 \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Rate\n",
    "\n",
    "Gauss Seidel method is known to be faster than Jacobi method (twice faster as stated in some textbooks), both of their convergence rate for our application is governed by the following theorem.\n",
    "\n",
    "$\\textbf{Theorem}\\;$\n",
    "The convergence rate of Jacobi and Gauss-Seidel method for the 5-point stencil finite difference method of the Poisson equation on a uniform mesh with size $h$ is\n",
    "\\begin{align*}\n",
    "1- \\mathcal{O}(h^2)\n",
    "\\end{align*}\n",
    "\n",
    "As such, the convergence rate stalls as the mesh gets finer. To alleviate the shortcoming of the numerical method, let's try improving the perforance by OpenMP! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.0\n",
    "Inspect the serial codebase [fd_laplace-serial.c](./fd_laplace-serial.c). Identify the two solvers **Jacobi** and **GaussSeidel**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the serial code\n",
    "!gcc -g -Wall -O3 -o fd_laplace-serial fd_laplace-serial.c -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobi METHOD IS IN USE\n",
      "Residual after 1000 iteration: 2792.5315167908\n",
      "Residual after 2000 iteration: 2642.5622881079\n",
      "Residual after 3000 iteration: 2500.6469594138\n",
      "Residual after 4000 iteration: 2366.3530066127\n",
      "Residual after 5000 iteration: 2239.2711337460\n",
      "Residual after 6000 iteration: 2119.0140255557\n",
      "Residual after 7000 iteration: 2005.2151670396\n",
      "Residual after 8000 iteration: 1897.5277264016\n",
      "Residual after 9000 iteration: 1795.6234979903\n",
      "Residual after 10000 iteration: 1699.1919020067\n",
      "Residual after 11000 iteration: 1607.9390379313\n",
      "Residual after 12000 iteration: 1521.5867887847\n",
      "Residual after 13000 iteration: 1439.8719734940\n",
      "Residual after 14000 iteration: 1362.5455447792\n",
      "Residual after 15000 iteration: 1289.3718301166\n",
      "Residual after 16000 iteration: 1220.1278134653\n",
      "Residual after 17000 iteration: 1154.6024555670\n",
      "Residual after 18000 iteration: 1092.5960507491\n",
      "Residual after 19000 iteration: 1033.9196182693\n",
      "Residual after 20000 iteration: 978.3943263470\n",
      "Residual after 21000 iteration: 925.8509471272\n",
      "Residual after 22000 iteration: 876.1293409140\n",
      "Residual after 23000 iteration: 829.0779681028\n",
      "Residual after 24000 iteration: 784.5534273244\n",
      "Residual after 25000 iteration: 742.4200183910\n",
      "Residual after 26000 iteration: 702.5493287149\n",
      "Residual after 27000 iteration: 664.8198419372\n",
      "Residual after 28000 iteration: 629.1165675752\n",
      "Residual after 29000 iteration: 595.3306905588\n",
      "Residual after 30000 iteration: 563.3592395877\n",
      "Residual after 31000 iteration: 533.1047732999\n",
      "Residual after 32000 iteration: 504.4750832934\n",
      "Residual after 33000 iteration: 477.3829130972\n",
      "Residual after 34000 iteration: 451.7456922339\n",
      "Residual after 35000 iteration: 427.4852845652\n",
      "Residual after 36000 iteration: 404.5277501510\n",
      "Residual after 37000 iteration: 382.8031198984\n",
      "Residual after 38000 iteration: 362.2451823126\n",
      "Residual after 39000 iteration: 342.7912816999\n",
      "Residual after 40000 iteration: 324.3821272081\n",
      "Residual after 41000 iteration: 306.9616121222\n",
      "Residual after 42000 iteration: 290.4766428647\n",
      "Residual after 43000 iteration: 274.8769771782\n",
      "Residual after 44000 iteration: 260.1150710002\n",
      "Residual after 45000 iteration: 246.1459335591\n",
      "Residual after 46000 iteration: 232.9269902535\n",
      "Residual after 47000 iteration: 220.4179528953\n",
      "Residual after 48000 iteration: 208.5806969199\n",
      "Residual after 49000 iteration: 197.3791451927\n",
      "Residual after 50000 iteration: 186.7791580539\n",
      "Residual after 51000 iteration: 176.7484292693\n",
      "Residual after 52000 iteration: 167.2563875684\n",
      "Residual after 53000 iteration: 158.2741034706\n",
      "Residual after 54000 iteration: 149.7742011149\n",
      "Residual after 55000 iteration: 141.7307748249\n",
      "Residual after 56000 iteration: 134.1193101544\n",
      "Residual after 57000 iteration: 126.9166091735\n",
      "Residual after 58000 iteration: 120.1007197662\n",
      "Residual after 59000 iteration: 113.6508687263\n",
      "Residual after 60000 iteration: 107.5473984451\n",
      "Residual after 61000 iteration: 101.7717070000\n",
      "Residual after 62000 iteration: 96.3061914602\n",
      "Residual after 63000 iteration: 91.1341942370\n",
      "Residual after 64000 iteration: 86.2399523155\n",
      "Residual after 65000 iteration: 81.6085492131\n",
      "Residual after 66000 iteration: 77.2258695170\n",
      "Residual after 67000 iteration: 73.0785558641\n",
      "Residual after 68000 iteration: 69.1539682309\n",
      "Residual after 69000 iteration: 65.4401454097\n",
      "Residual after 70000 iteration: 61.9257685538\n",
      "Residual after 71000 iteration: 58.6001266802\n",
      "Residual after 72000 iteration: 55.4530840251\n",
      "Residual after 73000 iteration: 52.4750491527\n",
      "Residual after 74000 iteration: 49.6569457224\n",
      "Residual after 75000 iteration: 46.9901848268\n",
      "Residual after 76000 iteration: 44.4666388143\n",
      "Residual after 77000 iteration: 42.0786165180\n",
      "Residual after 78000 iteration: 39.8188398153\n",
      "Residual after 79000 iteration: 37.6804214453\n",
      "Residual after 80000 iteration: 35.6568440188\n",
      "Residual after 81000 iteration: 33.7419401537\n",
      "Residual after 82000 iteration: 31.9298736797\n",
      "Residual after 83000 iteration: 30.2151218501\n",
      "Residual after 84000 iteration: 28.5924585100\n",
      "Residual after 85000 iteration: 27.0569381683\n",
      "Residual after 86000 iteration: 25.6038809249\n",
      "Residual after 87000 iteration: 24.2288582079\n",
      "Residual after 88000 iteration: 22.9276792757\n",
      "Residual after 89000 iteration: 21.6963784450\n",
      "Residual after 90000 iteration: 20.5312030043\n",
      "Residual after 91000 iteration: 19.4286017767\n",
      "Residual after 92000 iteration: 18.3852142965\n",
      "Residual after 93000 iteration: 17.3978605672\n",
      "Residual after 94000 iteration: 16.4635313701\n",
      "Residual after 95000 iteration: 15.5793790924\n",
      "Residual after 96000 iteration: 14.7427090489\n",
      "Residual after 97000 iteration: 13.9509712685\n",
      "Residual after 98000 iteration: 13.2017527232\n",
      "Residual after 99000 iteration: 12.4927699736\n",
      "Residual after 100000 iteration: 11.8218622092\n",
      "Residual after 101000 iteration: 11.1869846630\n",
      "Residual after 102000 iteration: 10.5862023796\n",
      "Residual after 103000 iteration: 10.0176843177\n",
      "Residual after 104000 iteration: 9.4796977699\n",
      "Residual after 105000 iteration: 8.9706030814\n",
      "Residual after 106000 iteration: 8.4888486529\n",
      "Residual after 107000 iteration: 8.0329662117\n",
      "Residual after 108000 iteration: 7.6015663369\n",
      "Residual after 109000 iteration: 7.1933342244\n",
      "Residual after 110000 iteration: 6.8070256801\n",
      "Residual after 111000 iteration: 6.4414633275\n",
      "Residual after 112000 iteration: 6.0955330198\n",
      "Residual after 113000 iteration: 5.7681804440\n",
      "Residual after 114000 iteration: 5.4584079071\n",
      "Residual after 115000 iteration: 5.1652712966\n",
      "Residual after 116000 iteration: 4.8878772018\n",
      "Residual after 117000 iteration: 4.6253801917\n",
      "Residual after 118000 iteration: 4.3769802379\n",
      "Residual after 119000 iteration: 4.1419202766\n",
      "Residual after 120000 iteration: 3.9194839011\n",
      "Residual after 121000 iteration: 3.7089931782\n",
      "Residual after 122000 iteration: 3.5098065826\n",
      "Residual after 123000 iteration: 3.3213170408\n",
      "Residual after 124000 iteration: 3.1429500817\n",
      "Residual after 125000 iteration: 2.9741620853\n",
      "Residual after 126000 iteration: 2.8144386261\n",
      "Residual after 127000 iteration: 2.6632929049\n",
      "Residual after 128000 iteration: 2.5202642658\n",
      "Residual after 129000 iteration: 2.3849167913\n",
      "Residual after 130000 iteration: 2.2568379748\n",
      "Residual after 131000 iteration: 2.1356374626\n",
      "Residual after 132000 iteration: 2.0209458643\n",
      "Residual after 133000 iteration: 1.9124136272\n",
      "Residual after 134000 iteration: 1.8097099711\n",
      "Residual after 135000 iteration: 1.7125218797\n",
      "Residual after 136000 iteration: 1.6205531467\n",
      "Residual after 137000 iteration: 1.5335234734\n",
      "Residual after 138000 iteration: 1.4511676142\n",
      "Residual after 139000 iteration: 1.3732345680\n",
      "Residual after 140000 iteration: 1.2994868134\n",
      "Residual after 141000 iteration: 1.2296995849\n",
      "Residual after 142000 iteration: 1.1636601876\n",
      "Residual after 143000 iteration: 1.1011673492\n",
      "Residual after 144000 iteration: 1.0420306064\n",
      "Residual after 145000 iteration: 0.9860697246\n",
      "Residual after 146000 iteration: 0.9331141482\n",
      "Residual after 147000 iteration: 0.8830024814\n",
      "Residual after 148000 iteration: 0.8355819956\n",
      "Residual after 149000 iteration: 0.7907081646\n",
      "Residual after 150000 iteration: 0.7482442236\n",
      "Residual after 151000 iteration: 0.7080607526\n",
      "Residual after 152000 iteration: 0.6700352820\n",
      "Residual after 153000 iteration: 0.6340519194\n",
      "Residual after 154000 iteration: 0.6000009958\n",
      "Residual after 155000 iteration: 0.5677787322\n",
      "Residual after 156000 iteration: 0.5372869229\n",
      "Residual after 157000 iteration: 0.5084326360\n",
      "Residual after 158000 iteration: 0.4811279307\n",
      "Residual after 159000 iteration: 0.4552895886\n",
      "Residual after 160000 iteration: 0.4308388606\n",
      "Residual after 161000 iteration: 0.4077012268\n",
      "Residual after 162000 iteration: 0.3858061692\n",
      "Residual after 163000 iteration: 0.3650869568\n",
      "Residual after 164000 iteration: 0.3454804425\n",
      "Residual after 165000 iteration: 0.3269268701\n",
      "Residual after 166000 iteration: 0.3093696930\n",
      "Residual after 167000 iteration: 0.2927554009\n",
      "Residual after 168000 iteration: 0.2770333575\n",
      "Residual after 169000 iteration: 0.2621556458\n",
      "Residual after 170000 iteration: 0.2480769220\n",
      "Residual after 171000 iteration: 0.2347542774\n",
      "Residual after 172000 iteration: 0.2221471081\n",
      "Residual after 173000 iteration: 0.2102169901\n",
      "Residual after 174000 iteration: 0.1989275634\n",
      "Residual after 175000 iteration: 0.1882444205\n",
      "Residual after 176000 iteration: 0.1781350017\n",
      "Residual after 177000 iteration: 0.1685684960\n",
      "Residual after 178000 iteration: 0.1595157468\n",
      "Residual after 179000 iteration: 0.1509491637\n",
      "Residual after 180000 iteration: 0.1428426375\n",
      "Residual after 181000 iteration: 0.1351714618\n",
      "Residual after 182000 iteration: 0.1279122563\n",
      "Residual after 183000 iteration: 0.1210428970\n",
      "Residual after 184000 iteration: 0.1145424476\n",
      "Residual after 185000 iteration: 0.1083910962\n",
      "Residual after 186000 iteration: 0.1025700951\n",
      "Residual after 187000 iteration: 0.0970617032\n",
      "Residual after 188000 iteration: 0.0918491322\n",
      "Residual after 189000 iteration: 0.0869164955\n",
      "Residual after 190000 iteration: 0.0822487595\n",
      "Residual after 191000 iteration: 0.0778316982\n",
      "Residual after 192000 iteration: 0.0736518493\n",
      "Residual after 193000 iteration: 0.0696964737\n",
      "Residual after 194000 iteration: 0.0659535163\n",
      "Residual after 195000 iteration: 0.0624115696\n",
      "Residual after 196000 iteration: 0.0590598384\n",
      "Residual after 197000 iteration: 0.0558881075\n",
      "Residual after 198000 iteration: 0.0528867103\n",
      "Residual after 199000 iteration: 0.0500464992\n",
      "Residual after 200000 iteration: 0.0473588178\n",
      "Residual after 201000 iteration: 0.0448154748\n",
      "Residual after 202000 iteration: 0.0424087188\n",
      "Residual after 203000 iteration: 0.0401312144\n",
      "Residual after 204000 iteration: 0.0379760203\n",
      "Residual after 205000 iteration: 0.0359365681\n",
      "Residual after 206000 iteration: 0.0340066421\n",
      "Residual after 207000 iteration: 0.0321803601\n",
      "Residual after 208000 iteration: 0.0304521563\n",
      "Residual after 209000 iteration: 0.0288167633\n",
      "Residual after 210000 iteration: 0.0272691970\n",
      "Residual after 211000 iteration: 0.0258047407\n",
      "Residual after 212000 iteration: 0.0244189310\n",
      "Residual after 213000 iteration: 0.0231075444\n",
      "Residual after 214000 iteration: 0.0218665842\n",
      "Residual after 215000 iteration: 0.0206922681\n",
      "Residual after 216000 iteration: 0.0195810171\n",
      "Residual after 217000 iteration: 0.0185294443\n",
      "Residual after 218000 iteration: 0.0175343449\n",
      "Residual after 219000 iteration: 0.0165926861\n",
      "Residual after 220000 iteration: 0.0157015977\n",
      "Residual after 221000 iteration: 0.0148583640\n",
      "Residual after 222000 iteration: 0.0140604152\n",
      "Residual after 223000 iteration: 0.0133053190\n",
      "Residual after 224000 iteration: 0.0125907745\n",
      "Residual after 225000 iteration: 0.0119146034\n",
      "Residual after 226000 iteration: 0.0112747453\n",
      "Residual after 227000 iteration: 0.0106692500\n",
      "Residual after 228000 iteration: 0.0100962720\n",
      "Residual after 229000 iteration: 0.0095540650\n",
      "Residual after 230000 iteration: 0.0090409765\n",
      "Residual after 231000 iteration: 0.0085554428\n",
      "Residual after 232000 iteration: 0.0080959840\n",
      "Residual after 233000 iteration: 0.0076611999\n",
      "Residual after 234000 iteration: 0.0072497652\n",
      "Residual after 235000 iteration: 0.0068604261\n",
      "Residual after 236000 iteration: 0.0064919959\n",
      "Residual after 237000 iteration: 0.0061433519\n",
      "Residual after 238000 iteration: 0.0058134312\n",
      "Residual after 239000 iteration: 0.0055012286\n",
      "Residual after 240000 iteration: 0.0052057924\n",
      "Residual after 241000 iteration: 0.0049262221\n",
      "Residual after 242000 iteration: 0.0046616658\n",
      "Residual after 243000 iteration: 0.0044113172\n",
      "Residual after 244000 iteration: 0.0041744132\n",
      "Residual after 245000 iteration: 0.0039502319\n",
      "Residual after 246000 iteration: 0.0037380899\n",
      "Residual after 247000 iteration: 0.0035373407\n",
      "Residual after 248000 iteration: 0.0033473724\n",
      "Residual after 249000 iteration: 0.0031676063\n",
      "Residual after 250000 iteration: 0.0029974941\n",
      "Residual after 251000 iteration: 0.0028365176\n",
      "Residual after 252000 iteration: 0.0026841862\n",
      "Residual after 253000 iteration: 0.0025400356\n",
      "Residual after 254000 iteration: 0.0024036262\n",
      "Residual after 255000 iteration: 0.0022745426\n",
      "Residual after 256000 iteration: 0.0021523913\n",
      "Residual after 257000 iteration: 0.0020368000\n",
      "Residual after 258000 iteration: 0.0019274163\n",
      "Residual after 259000 iteration: 0.0018239070\n",
      "Residual after 260000 iteration: 0.0017259565\n",
      "Residual after 261000 iteration: 0.0016332663\n",
      "Residual after 262000 iteration: 0.0015455538\n",
      "Residual after 263000 iteration: 0.0014625519\n",
      "Residual after 264000 iteration: 0.0013840075\n",
      "Residual after 265000 iteration: 0.0013096812\n",
      "Residual after 266000 iteration: 0.0012393465\n",
      "Residual after 267000 iteration: 0.0011727890\n",
      "Residual after 268000 iteration: 0.0011098059\n",
      "Residual after 269000 iteration: 0.0010502053\n",
      "Residual after 270000 iteration: 0.0009938054\n",
      "Residual after 271000 iteration: 0.0009404344\n",
      "Residual after 272000 iteration: 0.0008899296\n",
      "Residual after 273000 iteration: 0.0008421371\n",
      "Residual after 274000 iteration: 0.0007969112\n",
      "Residual after 275000 iteration: 0.0007541142\n",
      "Residual after 276000 iteration: 0.0007136154\n",
      "Residual after 277000 iteration: 0.0006752917\n",
      "Residual after 278000 iteration: 0.0006390260\n",
      "Residual after 279000 iteration: 0.0006047079\n",
      "Residual after 280000 iteration: 0.0005722329\n",
      "Residual after 281000 iteration: 0.0005415019\n",
      "Residual after 282000 iteration: 0.0005124213\n",
      "Residual after 283000 iteration: 0.0004849024\n",
      "Residual after 284000 iteration: 0.0004588613\n",
      "Residual after 285000 iteration: 0.0004342188\n",
      "Residual after 286000 iteration: 0.0004108996\n",
      "Residual after 287000 iteration: 0.0003888328\n",
      "Residual after 288000 iteration: 0.0003679511\n",
      "Residual after 289000 iteration: 0.0003481907\n",
      "Residual after 290000 iteration: 0.0003294916\n",
      "Residual after 291000 iteration: 0.0003117967\n",
      "Residual after 292000 iteration: 0.0002950521\n",
      "Residual after 293000 iteration: 0.0002792067\n",
      "Residual after 294000 iteration: 0.0002642123\n",
      "Residual after 295000 iteration: 0.0002500231\n",
      "Residual after 296000 iteration: 0.0002365959\n",
      "Residual after 297000 iteration: 0.0002238899\n",
      "Residual after 298000 iteration: 0.0002118662\n",
      "Residual after 299000 iteration: 0.0002004881\n",
      "Residual after 300000 iteration: 0.0001897212\n",
      "Residual after 301000 iteration: 0.0001795325\n",
      "Residual after 302000 iteration: 0.0001698909\n",
      "Residual after 303000 iteration: 0.0001607672\n",
      "Residual after 304000 iteration: 0.0001521334\n",
      "Residual after 305000 iteration: 0.0001439633\n",
      "Residual after 306000 iteration: 0.0001362319\n",
      "Residual after 307000 iteration: 0.0001289158\n",
      "Residual after 308000 iteration: 0.0001219925\n",
      "Residual after 309000 iteration: 0.0001154410\n",
      "Residual after 310000 iteration: 0.0001092415\n",
      "Residual after 311000 iteration: 0.0001033748\n",
      "Residual after 312000 iteration: 0.0000978232\n",
      "Residual after 313000 iteration: 0.0000925697\n",
      "Residual after 314000 iteration: 0.0000875984\n",
      "Residual after 315000 iteration: 0.0000828940\n",
      "Residual after 316000 iteration: 0.0000784423\n",
      "Residual after 317000 iteration: 0.0000742297\n",
      "Residual after 318000 iteration: 0.0000702433\n",
      "Residual after 319000 iteration: 0.0000664709\n",
      "Residual after 320000 iteration: 0.0000629012\n",
      "Residual after 321000 iteration: 0.0000595231\n",
      "Residual after 322000 iteration: 0.0000563265\n",
      "Residual after 323000 iteration: 0.0000533016\n",
      "Residual after 324000 iteration: 0.0000504391\n",
      "Residual after 325000 iteration: 0.0000477303\n",
      "Residual after 326000 iteration: 0.0000451671\n",
      "Residual after 327000 iteration: 0.0000427414\n",
      "Residual after 328000 iteration: 0.0000404461\n",
      "Residual after 329000 iteration: 0.0000382739\n",
      "Residual after 330000 iteration: 0.0000362185\n",
      "Residual after 331000 iteration: 0.0000342734\n",
      "Residual after 332000 iteration: 0.0000324328\n",
      "Residual after 333000 iteration: 0.0000306911\n",
      "Residual after 334000 iteration: 0.0000290428\n",
      "Residual after 335000 iteration: 0.0000274831\n",
      "Residual after 336000 iteration: 0.0000260072\n",
      "Residual after 337000 iteration: 0.0000246105\n",
      "Residual after 338000 iteration: 0.0000232889\n",
      "Residual after 339000 iteration: 0.0000220382\n",
      "Residual after 340000 iteration: 0.0000208546\n",
      "Residual after 341000 iteration: 0.0000197347\n",
      "Residual after 342000 iteration: 0.0000186748\n",
      "Residual after 343000 iteration: 0.0000176719\n",
      "Residual after 344000 iteration: 0.0000167229\n",
      "Residual after 345000 iteration: 0.0000158248\n",
      "Residual after 346000 iteration: 0.0000149749\n",
      "Residual after 347000 iteration: 0.0000141708\n",
      "Residual after 348000 iteration: 0.0000134098\n",
      "Residual after 349000 iteration: 0.0000126896\n",
      "Residual after 350000 iteration: 0.0000120081\n",
      "Residual after 351000 iteration: 0.0000113632\n",
      "Residual after 352000 iteration: 0.0000107530\n",
      "Residual after 353000 iteration: 0.0000101755\n",
      "Residual after 354000 iteration: 0.0000096290\n"
     ]
    }
   ],
   "source": [
    "# Execute the serial code with Jacobi method to solve on a grid of 300 x 300 meshes i.e. matrix size 90000 x 90000 at stopping criterion 1e-5.\n",
    "!./fd_laplace-serial 300 1e-5 Jacobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Verify the results by plot\n",
    "%run  'Laplace-plot.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1\n",
    "Add appropriate work-sharing loop construct and clauses in the **Jacobi** function provided in [fd_laplace-omp.c](./fd_laplace-omp.c) to accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the parallel code\n",
    "\n",
    "!gcc -fopenmp -g -Wall -O3 -o fd_laplace-omp fd_laplace-omp.c -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the parallel code with Jacobi method to solve on a grid of 300 x 300 meshes i.e. matrix size 90000 x 90000 at stopping criterion 1e-5.\n",
    "\n",
    "!OMP_NUM_THREADS=4 ./fd_laplace-omp 300 1e-5 Jacobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Verify the results by plotting\n",
    "%run  'Laplace-plot.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add appropriate work-sharing loop construct and clauses in the **GaussSeidel** function provided in [fd_laplace-omp.c](./fd_laplace-omp.c) to accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompile the parallel code again if you changed the Gauss-Seidel method\n",
    "\n",
    "!gcc -fopenmp -g -Wall -O3 -o fd_laplace-omp fd_laplace-omp.c -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the parallel code with Gauss-Seidel method to solve on a grid of 300 x 300 meshes i.e. matrix size 90000 x 90000 at stopping criterion 1e-5.\n",
    "\n",
    "!OMP_NUM_THREADS=4 ./fd_laplace-omp 300 1e-5 Gauss-Seidel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Verify the results by plot\n",
    "%run  'Laplace-plot.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway from Exercise 4\n",
    "\n",
    "Don’t optimize prematurely. First choose most suitable primitive algorithms; then consider parallelisation and tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

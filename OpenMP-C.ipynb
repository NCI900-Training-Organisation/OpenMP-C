{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Welcome to the Parallel C OpenMP hand-on session. \n",
    "\n",
    "#### In this Jupyter Notebook, you are provided with four scientific programming tasks that can be accelerated by OpenMP multithreading. Each example highlights a distinct loop-parallelisation theme and showcases different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "# the jupyter notebook is launched from your $HOME, change the working directory provided a username directory is created under /scratch/vp91\n",
    "os.chdir(os.path.expandvars(\"/scratch/vp91/$USER/OpenMP-C\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1. (Independent Loop Iterations) Monte-Carlo $\\pi$\n",
    "\n",
    "Consider the quarter of a unit circle inscribed in the unit square $[0,1]^2$. Draw $N$ points uniformly at random in the square, and let $h$ be the number that falls inside the quarter circle $(x^2+y^2\\le 1)$. By the area ratio,\n",
    "\\begin{align*}\n",
    "\\hat{\\pi} \\approx \\frac{4h}{N}.\n",
    "\\end{align*}\n",
    "\n",
    "Increasing $N$ reduces the estimator’s variance (error estimate is $O(N^{-1/2})$). Because each sample is independent, this Monte Carlo approach is *embarrassingly parallel*, making it an ideal example for OpenMP Multithreading paralelisation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the python-based Jupyter environment you are working on, we need to prepend lines of code with `!` to indicate we wish to execute a shell command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.0\n",
    "Inspect the serial codebase [monte-carlo-pi-serial.c](./monte-carlo-pi-serial.c). Idenity the for-loop where OpenMP multithreading is suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilee the serial code\n",
    "!gcc -g -Wall -O3 -o monte-carlo-pi-serial monte-carlo-pi-serial.c -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Execute the serial code\n",
    "!./monte-carlo-pi-serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: \n",
    "Add OpenMP work-sharing loop constructs and appropriate clauses to [monte-carlo-pi-omp.c](./monte-carlo-pi-omp.c) to parallelise the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the openmp code\n",
    "!gcc -fopenmp -g -Wall -O3 -o monte-carlo-pi-omp monte-carlo-pi-omp.c -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Execute the openmp code, compare the time consumption with the serial version\n",
    "!OMP_NUM_THREADS=4 ./monte-carlo-pi-omp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway from Exercise 1:\n",
    "If your workload is embarrassingly parallel (independent iterations, little/no communication), OpenMP is an excellent first choice on a single shared-memory machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2 (Dynamic Loop Iterations) Mandelbrot Set\n",
    "## Mandelbrot Set\n",
    "\n",
    "The **Mandelbrot set** is the set of complex numbers $c \\in \\mathbb{C}$ for which the sequence\n",
    "\n",
    "$$z_{n+1} = z_n^2 + c,\\qquad z_0 = 0$$\n",
    "remains **bounded** (does not diverge to infinity).\n",
    "\n",
    "We write\n",
    "$$\n",
    "M \\;=\\; \\Big\\{\\, c \\in \\mathbb{C} \\;\\big|\\; \\{z_n\\}_{n\\ge 0}\\ \\text{is bounded for } z_{n+1}=z_n^2+c,\\ z_0=0 \\,\\Big\\}.\n",
    "$$\n",
    "\n",
    "### Convergence (Escape) Test\n",
    "For visualization, we sample $c$ on a uniform grid over the rectangle\n",
    "$$\n",
    "[-2.0,\\ 0.47] \\times [-1.12,\\ 1.12]\\,i,\n",
    "$$\n",
    "which covers the most visually interesting portion of $M$.\n",
    "\n",
    "For each grid point $c$, iterate the map up to $N_{\\max}=100$ steps:\n",
    "$$\n",
    "z_{n+1} = z_n^2 + c,\\quad n=0,\\dots, N_{\\max}-1.\n",
    "$$\n",
    "If at any step $|z_n| > 2$ (equivalently $|z_n|^2 > 4$), the orbit is guaranteed to diverge and $c \\notin M$. If $|z_n|$ never exceeds the value $2$ within $N_{\\max}$ steps, we treat $c$ as (numerically) belonging to $M$.\n",
    "\n",
    "### Dynamic Workload\n",
    "Different points require different numbers of iterations before diverging (some escape in a few steps, others never escape within $N_{\\max}$). This **variable iteration count** creates load imbalance in parallel implementations. To mitigate this with OpenMP, use **dynamic scheduling** (e.g., `schedule(dynamic, chunk)`) so threads pick up new points as they finish, improving overall utilization.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.0:\n",
    "Inspect the serial codebase [mandelbrot-serial.c](./mandelbrot-serial.c).\n",
    "Compile and run the serial version of mandelbrot set generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the serial code\n",
    "\n",
    "!gcc -Wall -O3 mandelbrot-serial.c -o mandelbrot-serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the serial program \n",
    "!./mandelbrot-serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mandelbrot set to verify outputs\n",
    "%run  'Mandelbrot-plot.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "Add OpenMP work-sharing loop constructs and appropriate clauses to [mandelbrot-omp.c](./mandelbrot-omp.c) to parallelise the computation. Ensure that writes to the output file remain in order (e.g., with #pragma omp ordered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the openmp code\n",
    "!gcc -fopenmp -g -Wall -O3 -o mandelbrot-omp mandelbrot-omp.c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Execute the openmp code\n",
    "\n",
    "!OMP_NUM_THREADS=4  ./mandelbrot-omp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Verify the results by plot\n",
    "%run  'Mandelbrot-plot.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve probably noticed the OpenMP version doesn’t run faster. The culprit is the use of `ordered` around your output.\n",
    "\n",
    "**What `ordered` does:**  \n",
    "`ordered` enforces that a specific block inside a `parallel for` executes **in the original loop order**. Only one iteration at a time may enter that block. In other words, it **serializes** the part it wraps.\n",
    "\n",
    "**What to do instead (best practice)**\n",
    "- **Keep compute parallel, do I/O serially.**  \n",
    "  Compute the iteration counts into a preallocated buffer in the `parallel for` and write the data after the parallel region.\n",
    "\n",
    "\n",
    "### Exercise 2.2 \n",
    "Modify [mandelbrot-separateIO.c](./mandelbrot-separateIO.c) to separate the I/O from the computation routine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the openmp code after IO separation\n",
    "!gcc -fopenmp -g -Wall -O3 -o mandelbrot-separateIO mandelbrot-separateIO.c \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    " \n",
    "!OMP_NUM_THREADS=4  ./mandelbrot-separateIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the output again by plot\n",
    "%run  'Mandelbrot-plot.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway from Exercise 2:\n",
    "When planning acceleration, consider refactoring the code to fit your parallelisation tools. Here, we separated I/O from computation so the compute kernel could be parallelised cleanly with OpenMP (no I/O inside parallel regions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #3 (Loop Dependence). Solve Linear Equation by Conjugate Gradient Method \n",
    "\n",
    "The Conjugate Gradient method is a numerical method that is wildely used in solving certain type of matrix problems. It is also the base ingredient of [HPCG Benchmark](https://www.hpcg-benchmark.org/) for ranking HPC systems.\n",
    "\n",
    "\n",
    "Consider solving a linear equation\n",
    "\\begin{align*}\n",
    "Ax = b\n",
    "\\end{align*}\n",
    "where matrix $A \\in \\mathbf{R}^{n\\times n}$ is symmetric positive definite. \n",
    "\n",
    "The initial guess $x_0$ can be any approximation, we choose $0$. The baseline algorithm is statedd as following:\n",
    "\n",
    "Compute $r_0 = b - Ax_0$\n",
    "\n",
    "For $i= 0, \\cdots, n$ Do \n",
    "\n",
    " $\\alpha_i := (r_i, r_i)/(Ap_i, p_i)$\\;\n",
    " \n",
    " $x_{i+1}:=x_i+\\alpha_i p_i$\\;\n",
    "\n",
    " $r_{i+1}:=r_i -\\alpha_i Ap_i$\\;\n",
    " \n",
    " If $r_{i+1} <\\text{tolerance}$ Then Break\n",
    " \n",
    " $\\beta_i:= (r_{i+1}, r_{i+1}) / (r_i, r_i)$\\;\n",
    " \n",
    " $p_{i+1}:= r_{i+1} +\\beta_i p_i$\n",
    "\n",
    "Conjugate Gradient (CG) method is a direct method that produces the exact solution at most $n$ steps, however, in practice a tolerance is usually set to terminate iterations.\n",
    "\n",
    "CG method guarantees convergence for symmetric positive definite matrices in theory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelisation in CG method\n",
    "Note that the for-loop in CG method possesses the dependence between iterations. With multiple threads, each iteration can only be execute only after the dependence is met leading to a non-parallelizable loop. \n",
    "\n",
    "That being said, we can still use OpenMP to parallelise part of the code after analysing the bottleneck of its performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Matrices\n",
    "For numerical experiments, we will test two matrices [Trefethen_20](https://www.cise.ufl.edu/research/sparse/matrices/JGD_Trefethen/Trefethen_20.html) and [Msc04515](https://www.cise.ufl.edu/research/sparse/matrices/Boeing/msc04515.html).\n",
    "\n",
    "Trefethen_20 is a small-sized problem in which you should see a fast convergence.\n",
    "\n",
    "Msc04515 is a real-life problem arising from a structural engineering. It is an ill-conditioned matrix, which essentially means hard to solve and requires a lot more iterations for CG method if it converges at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.0\n",
    "Inspect the serial code base [cg-std.c](./cg-std.c), and map each step of the pseudo-algorithm to its corresponding implementation in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the serial code\n",
    "\n",
    "!gcc -g -Wall -O3 -o cg-std cg-std.c -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the serial program to solve Trefethen_20 problem\n",
    "!./cg-std 1e-5 < Trefethen_20.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "Add the appropriate OpenMP work-sharing constructs and clauses to the loops in [cg-std-omp.c](./cg-std-omp.c) to  to parallelise execution and improve performance.\n",
    "\n",
    "Once you are one, compile the code with the following command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the openmp code\n",
    "!gcc -fopenmp -g -Wall -O3 -o cg-std-omp cg-std-omp.c -lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code with a simple matrix [Trefethen_20.dat](./Trefethen_20.dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Verify the numerical result with the serial version\n",
    "!OMP_NUM_THREADS=4  ./cg-std-omp 1e-5 < Trefethen_20.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the result looks good, test it with a more serious matrix [msc04515.dat](./msc04515.dat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Execute the parallel code to solve Boeing msc04515 problem. You may also want to attempt this problem with the serial code\n",
    "\n",
    "!OMP_NUM_THREADS=4 ./cg-std-omp 1e-5 < msc04515.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results with a serial code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Execute the parallel code to solve Boeing msc04515 problem. You may also want to attempt this problem with the serial code\n",
    "\n",
    "!OMP_NUM_THREADS=4 ./cg-std 1e-5 < msc04515.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway from Exercise 3:\n",
    "You don’t always need to understand the entire codebase (though it often helps). Even in a large project, pinpointing the bottleneck and adding targeted parallelism can deliver significant speedups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #4 (Parallelisaton vs. Convergence Rate): Solve Finite Difference Discretised Poisson Equation by Jacobi and Gauss-Seidel Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Problem\n",
    "Consider a 2D Poisson equation with Dirichlet boundary condition over a unit square domain $\\Omega = [0,1] \\times [0,1]$\n",
    "\n",
    "\\begin{align*}\n",
    "-\\Delta u &= f \\; \\text{in} \\; \\Omega \\\\\n",
    " u &= g \\; \\text{on} \\; \\partial \\Omega\n",
    "\\end{align*}\n",
    "\n",
    "Define a uniform partition of the domain $\\Omega$ with nodal points at which the solution of the Poisson equation is sampled. Let $h$ be the uniform distance between two nodal points then the nodal points that lie on the mesh are defined by\n",
    "\n",
    "\\begin{align*}\n",
    "x_i = i h, \\; y_j = j h\\qquad i,j = 0,\\cdots, N\n",
    "\\end{align*}\n",
    "where $N$ is a given mesh size and $i, j$ are integers along $x, y$-axis telling the location of each nodal point.  \n",
    "\n",
    "\n",
    "### Discretisation\n",
    "We use the second-order central finite difference method to discretise the Laplace operator\n",
    "\\begin{align*}\n",
    "(\\Delta u)_{i,j}\n",
    "  &= \\bigl(D_{xx}^2 u\\bigr)_{i,j} + \\bigl(D_{yy}^2 u\\bigr)_{i,j} \\\\[2ex]\n",
    "  &\\approx \\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{h^2}\n",
    "          + \\frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{h^2}.\n",
    "\\end{align*}\n",
    "leading to\n",
    "\\begin{align}\n",
    "    -(\\Delta u)_{i,j} = \\frac{4u_{i,j}-u_{i+1,j}-u_{i-1,j}-u_{i,j+1}-u_{i,j-1}}{h^2}=f(u_{i,j}).\n",
    "\\end{align}\n",
    "\n",
    "The above finite-difference formula can further be represented by a five-point stencil matrix built in the mesh\n",
    "\\begin{align*}S = \n",
    "    \\begin{pmatrix}\n",
    "    & -1 & \\\\\n",
    "    -1 & 4 &-1\\\\\n",
    "    & -1 &\n",
    "    \\end{pmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "Impose the Dirichlet boundary condition, on the interior nodal points the discretisation can be written as a linear equation\n",
    "\n",
    "\\begin{align*}\n",
    "A u = f, \\qquad A=\\frac{1}{h^2}\n",
    "    \\begin{pmatrix}\n",
    "S & I \\\\\n",
    "I & S & I \\\\\n",
    "& I & \\ddots & \\ddots \\\\\n",
    "& & \\ddots & \\ddots & I \\\\\n",
    "& & & I & S\n",
    "\\end{pmatrix}\n",
    "\\end{align*}\n",
    "where $A \\in \\mathbb{R}^{(N-2)^2 \\times (N-2)^2}$, $u \\in \\mathbb{R}^{(N-2)^2}$ and $f \\in \\mathbb{R}^{(N-2)^2}$.\n",
    "\n",
    "Note that with the five-point stencil, the matrix $A$ was never assembled and is nowhere in sight! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Solvers\n",
    "\n",
    "To solve the linear system, two iterative methods are used and compared. \n",
    "\n",
    "$\\textbf{Jacobi method}$\n",
    "\n",
    "\\begin{align*}\n",
    "u^{(k+1)} = D^{-1}( f - Lu^{k} -Uu^k),\n",
    "\\end{align*}\n",
    "where $D, L, U$ are the diagonal matrix, lower triangular matrix and upper triangular matrix of $A$, respectively.\n",
    "Write into stecil,\n",
    "\\begin{align*}\n",
    "u^{(k+1)}_{ij} = (h^2 f_{ij} + u^{(k)}_{i-1,j} +u^{(k)}_{i+1,j} + u^{(k)}_{i, j-1}+u^{(k)}_{i,j+1})/4 \\qquad i,j = 1,\\cdots, N-1 \n",
    "\\end{align*} \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Gauss-Seidel method}$ follows a similar scheme:\n",
    "\n",
    "\\begin{align*}\n",
    "u^{(k+1)} = D^{-1} (f - L u_{k+1}- U u_k),\n",
    "\\end{align*}\n",
    "\n",
    "Note that Gauss-Seidel method uses the most recent estimate to update.\n",
    " \n",
    "Likewise, applying the Gauss-Seidel method doesn't require assembling $D, L, U$ matrices for finite-difference discretised Laplacian. \n",
    "Elementwise, we have \n",
    "\\begin{align*}\n",
    "u^{(k+1)}_{ij} = (h^2 f_{ij} + u^{(k)}_{i-1,j} +u^{(k+1)}_{i+1,j} + u^{(k+1)}_{i, j-1}+u^{(k)}_{i,j+1})/4 \\qquad i,j = 1,\\cdots, N-1 \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Rate\n",
    "\n",
    "Gauss Seidel method is known to be faster than Jacobi method (twice faster as stated in some textbooks), both of their convergence rate for our application is governed by the following theorem.\n",
    "\n",
    "$\\textbf{Theorem}\\;$\n",
    "The convergence rate of Jacobi and Gauss-Seidel method for the 5-point stencil finite difference method of the Poisson equation on a uniform mesh with size $h$ is\n",
    "\\begin{align*}\n",
    "1- \\mathcal{O}(h^2)\n",
    "\\end{align*}\n",
    "\n",
    "As such, the convergence rate stalls as the mesh gets finer. To alleviate the shortcoming of the numerical method, let's try improving the perforance by OpenMP! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.0\n",
    "Inspect the serial codebase [fd_laplace-serial.c](./fd_laplace-serial.c). Identify the two solvers **Jacobi** and **GaussSeidel**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the serial code\n",
    "!gcc -g -Wall -O3 -o fd_laplace-serial fd_laplace-serial.c -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the serial code with Jacobi method to solve on a grid of 300 x 300 meshes i.e. matrix size 90000 x 90000 at stopping criterion 1e-5.\n",
    "!./fd_laplace-serial 300 1e-5 Jacobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Verify the results by plot\n",
    "%run  'Laplace-plot.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1\n",
    "Add appropriate work-sharing loop construct and clauses in the **Jacobi** function provided in [fd_laplace-omp.c](./fd_laplace-omp.c) to accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the parallel code\n",
    "\n",
    "!gcc -fopenmp -g -Wall -O3 -o fd_laplace-omp fd_laplace-omp.c -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the parallel code with Jacobi method to solve on a grid of 300 x 300 meshes i.e. matrix size 90000 x 90000 at stopping criterion 1e-5.\n",
    "\n",
    "!OMP_NUM_THREADS=4 ./fd_laplace-omp 300 1e-5 Jacobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Verify the results by plotting\n",
    "%run  'Laplace-plot.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add appropriate work-sharing loop construct and clauses in the **GaussSeidel** function provided in [fd_laplace-omp.c](./fd_laplace-omp.c) to accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompile the parallel code again if you changed the Gauss-Seidel method\n",
    "\n",
    "!gcc -fopenmp -g -Wall -O3 -o fd_laplace-omp fd_laplace-omp.c -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the parallel code with Gauss-Seidel method to solve on a grid of 300 x 300 meshes i.e. matrix size 90000 x 90000 at stopping criterion 1e-5.\n",
    "\n",
    "!OMP_NUM_THREADS=4 ./fd_laplace-omp 300 1e-5 Gauss-Seidel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Verify the results by plot\n",
    "%run  'Laplace-plot.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway from Exercise 4\n",
    "\n",
    "Don’t optimize prematurely. First choose most suitable primitive algorithms; then consider parallelisation and tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
